{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automatic-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "happy-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/NASA_Transfer_Learning/toy')\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aerial-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/NASA_Transfer_Learning/\") # Add root directory to PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preliminary-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from datasets import mnist, mnist_m\n",
    "from models.ganin import SimpleClassifier\n",
    "import trainers.ganin as trainer\n",
    "# from trainer import train_ganin, test_ganin # getting called within /toy\n",
    "from utils import transform, helper\n",
    "\n",
    "\n",
    "# Set random seed to ensure deterministic behavior\n",
    "helper.set_random_seed(seed=123)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuous-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wandb ()\n",
    "\n",
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "velvet-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "responsible-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "config = dict(epochs=15,\n",
    "              batch_size=64,\n",
    "              learning_rate=2e-4,\n",
    "              classes=10,\n",
    "              img_size=28,\n",
    "              dataset='mnist-mnist_m',\n",
    "              architecture='ganin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "textile-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hyperparameters):\n",
    "\n",
    "    with wandb.init(project=\"uda-ganin-toy\", config=hyperparameters, mode='online'):\n",
    "        model = SimpleClassifier().to(device)\n",
    "        wandb.watch(model, log='all', log_freq=100)\n",
    "\n",
    "        # transforms\n",
    "        transform_m = transform.get_transform(dataset=\"mnist\")\n",
    "        transform_mm = transform.get_transform(dataset=\"mnist_m\")\n",
    "\n",
    "        # dataloaders\n",
    "        loaders_args = dict(\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        train_src = torch.load(\"../data/mnist/processed/train.pt\")\n",
    "        trainloader_src = mnist.fetch(data=train_src,\n",
    "                                    transform=transform_m,\n",
    "                                    **loaders_args)\n",
    "\n",
    "        # fetching testloader_m for symmetry but it is not needed in the code\n",
    "        test_src = torch.load(\"../data/mnist/processed/test.pt\")\n",
    "        testloader_src = mnist.fetch(data=test_src,\n",
    "                                transform=transform_m,\n",
    "                                **loaders_args)\n",
    "\n",
    "        train_tgt = torch.load(\"../data/mnist_m/processed/train.pt\")\n",
    "        trainloader_tgt = mnist_m.fetch(data=train_tgt,\n",
    "                                    transform=transform_mm,\n",
    "                                    **loaders_args)\n",
    "\n",
    "        test_tgt = torch.load(\"../data/mnist_m/processed/test.pt\")\n",
    "        testloader_tgt = mnist_m.fetch(data=test_tgt,\n",
    "                                    transform=transform_mm,\n",
    "                                    **loaders_args)\n",
    "        \n",
    "\n",
    "        criterion = nn.CrossEntropyLoss() # class labels 1 to N class\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        for epoch in range(1, config[\"epochs\"]+1):\n",
    "            \n",
    "            loss = trainer.train_simple_classifier(model, epoch, config, trainloader_src,\n",
    "                        criterion, optimizer, device)\n",
    "            \n",
    "            wandb.log({\"epoch\" : epoch, \"train_loss\" : loss}, step=epoch)\n",
    "\n",
    "            accuracy = trainer.test_simple_classifier(model, testloader_tgt, device)\n",
    "\n",
    "            wandb.log({\"epoch\" : epoch, \"val_accuracy\" : accuracy}, step=epoch)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        print(f\"Train Time for {config['epochs']} epochs: {end_time - start_time}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "considered-afternoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.24<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">vague-sponge-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nisyad/uda-ganin-toy\" target=\"_blank\">https://wandb.ai/nisyad/uda-ganin-toy</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nisyad/uda-ganin-toy/runs/1ud6iva1\" target=\"_blank\">https://wandb.ai/nisyad/uda-ganin-toy/runs/1ud6iva1</a><br/>\n",
       "                Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/NASA_Transfer_Learning/toy/wandb/run-20210403_013356-1ud6iva1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
